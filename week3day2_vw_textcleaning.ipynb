{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fundamentals of Social Data Science\n",
    "# Week 3 Day 2: Natural Language Processing \n",
    "\n",
    "In this class we will be exposed to some of the basics of natural lagnuage processing. This is an incredibly deep field for which we can only scratch the surface. It is also a field that has had some very close ties to some of the most impressive advances in artificial intelligence and machine learning. In this case, we will not be focusing with any depth on the AI/ML consequences of this work, but will instead focus on some foundational topics that will be incredibly useful to appreciate in the run up to an understanding of machine learning. These foundational topics point to the _motivations_ for text analysis as well as the considerations with textual data. \n",
    "\n",
    "Learning goals: \n",
    "\n",
    "- Appreciate text encodings \n",
    "- Within English, appreciate tokenisation, stemming, lemmatisation, stop words\n",
    "- Understand how to strip HTML from text\n",
    "- Understand how to create a Term Frequency-Inverse Document Frequency matrix. \n",
    "\n",
    "This lecture draws on code from FSSTDS chapters 10 and 11, self-prompted code from Claude Sonnet 3.5 (original and new), ChatGPT, and general online sources as well as bespoke written code. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter \n",
    "# Counter is like a value counts for lists\n",
    "# It returns a dictionary with the count of each element in the list\n",
    "# where value_counts returns a pandas series with the count of each element in the series\n",
    "\n",
    "import re\n",
    "import string \n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer, WordNetLemmatizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Encodings: Considering Text and Language\n",
    "\n",
    "Language is what people speak as well as what they understand when they read. However, people do not read languages they read text. They do not hear languages, they hear speech. In both cases, language refers to a set of signs that have meaning, but (and this comes from semiotics), we never observe signs directly. Instead, they are a _relation_ between what we call a signifier and a signified. The signifier might be the word \"horse\", the signified might be an actual equine animal grazing in a field. The sign \"horse\" is the semantic association we make between the word \"horse\" (whether read on a sign \"beware of wild horses\" or in speech, where someone says \"did you notice the horse in that field over there\"). \n",
    "\n",
    "Within natural language processing, we are interested in these signs (i.e. what does a 'horse' mean?) and how we can understand them from the data we collect (\"How has our discussion of horses changed after we started driving cars\" or \"do animal rights groups and jockeys use the same set of associations when talking about horses\"). But while we are interested in these signs (i.e. the semantic elements of language), we cannot access them directly. They are implicit. Instead we access _data_ which in this case typically means text. NLP can also consider other metatextual elements such as speed, pitch, emphasis, differences in dialect or spelling. But typically we are focused on text and how that text helps us to understand the world better, or how we can use tools in NLP to understand a text better. \n",
    "\n",
    "When we translate language into text we need to encode it somehow. So instead of me saying <audio controls>\n",
    "    <source src=\"audio_sample.m4a\" type=\"audio/mpeg\">\n",
    "    Your browser does not support the audio element.\n",
    "</audio>, we read \"Hello everyone\". \n",
    "\n",
    "Text is therefore an encoding of language. But it is an encoding that is useful for humans. Computers on the other hand need ways to encode text. Thus, before we even get to the analysis of text it is worth appreciating encodings if only superficially. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a 97\n",
      "A 65\n",
      "üòÇ 128514\n"
     ]
    }
   ],
   "source": [
    "print(\"a\", ord(\"a\"))\n",
    "print(\"A\", ord(\"A\"))\n",
    "print(\"üòÇ\",ord(\"üòÇ\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The way that the computer sees these characters is through these code points. But they are not encoded as numbers like this. They are converted into numbers using 'code points' in a system called Unicode. Below we can see how we can convert these numbers into Unicode strings and see how the computer decodes them as emojis. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Character: üòÇ\n",
      "Ord value: 128514\n",
      "Unicode escape: \\U0001f602\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "char = \"üòÇ\"\n",
    "code_point = ord(char)\n",
    "\n",
    "# Convert to Unicode escape sequence\n",
    "if code_point > 0xFFFF:\n",
    "    # For characters above U+FFFF, use \\U format with 8 digits\n",
    "    unicode_escape = f\"\\\\U{code_point:08x}\"\n",
    "else:\n",
    "    # For characters U+0000 to U+FFFF, use \\u format with 4 digits\n",
    "    unicode_escape = f\"\\\\u{code_point:04x}\"\n",
    "\n",
    "print(f\"Character: {char}\")\n",
    "print(f\"Ord value: {code_point}\")\n",
    "print(f\"Unicode escape: {unicode_escape}\")\n",
    "\n",
    "# Verify it works\n",
    "print(eval(f'\"{unicode_escape}\"') == char)  # True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can work our way backwards and see how the Unicode escape codes can then be interpreted by the computer so we get our emoji back. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üòÇ\n",
      "üòÇ\n",
      "True\n",
      "True\n"
     ]
    }
   ],
   "source": [
    "# Unicode escape sequence\n",
    "emoji_unicode = \"\\U0001F602\"  # Note the capital U and 8 digits for characters above U+FFFF\n",
    "print(emoji_unicode)  # üòÇ\n",
    "\n",
    "# UTF-8 bytes as escape sequences\n",
    "emoji_bytes = b\"\\xf0\\x9f\\x98\\x82\"\n",
    "print(emoji_bytes.decode('utf-8'))  # üòÇ\n",
    "\n",
    "# Verify they're the same\n",
    "print(emoji_unicode == \"üòÇ\")  # True\n",
    "print(emoji_bytes.decode('utf-8') == \"üòÇ\")  # True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There may be times in your work when you get encode and decode errors when reading text or strings. Remember, in this sense, encode means for the computer so it would have the byte strings whereas decode means for you the interpreter of this data and would be in the text and emoji strings you would expect to read. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing and \"Pre-Processing\" Text \n",
    "\n",
    "In order to work with text we need to somehow appreciate its _context_. That context can vary from 'the other words around this text'. For example in a humourous episode of The Simpsons, some aliens abduct the family and have them on board the ship. The plot follows the concerns that the Simpsons are going to be on the menu. Daughter Lisa discovers a book and warns the rest of the family. The book says \"How to Cook Humans\". The family confront the aliens who then dust the book revealing it is \"How to Cook For Humans\". Then one of the Simpsons blows off more dust to reveal \"How to Cook Forty Humans\" followed by \"How to Cook for Forty Humans\". See clip at: https://www.youtube.com/watch?v=o0QcdgeI5Rs In each exchange between Kang the Alien and Lisa, the semantics of the word \"humans\" changes depending on the word in front of it, either as alien meal or as guests to a party. \n",
    "\n",
    "How we understand the context for a text is a challenging and expansive subject of inquiry. There are many approaches to this and they range greatly in sophistication. Below is a crude scale of context: \n",
    "\n",
    "0. **No context**. We can think of words as separate semantic entities. (i.e \"Bag of words\")\n",
    "1. **Adjacent words**. Words will have words before and after them. Combining these together we get N-grams. So \"cook humans\", \"for humans\", and \"forty humans\" would all be 2-grams. \n",
    "2. **Adjacent words as blocks of text**. One does not need to specify the exact number of n-grams. A sentence could be short or long, but it is a single sentence and thus a single collection of words that typically mean things when read together. What is important is the sensemaking that occurs through grammar. In english, consider the sentences \"The colour paints well\" and \"The paints colour well\". Here the order gives us a signal that colour is a noun and a subject in the first case, but a verb in the second case. \n",
    "3. **Words-in-documents**. Some documents will use some rare words a lot while most documents use some common words. Simply knowing that two words appear in the same document is a useful context. It is also the basis of TF-IDF which we will see at the end. \n",
    "4. **Words and their performance**. Words can be uttered as speech. This means that not only will other words be the context, but we might also have metatextual features like pitch. Referencing another pop culture moment, in the sitcom friends, there was an extended discussion about leaving the house when two of the cast forgot their keys. Monica leaves the house asking Rachel behind her \"got the keys?\" https://youtu.be/JjpnslsuA2g?t=30 . When they get back Rachel said Monica don't you have the keys asserting \"You said 'got the keys'\" as if it was an assertion not a question. Thus, the lack of attention to the context of pitch and tone left them with a misunderstanding that locked them out of the house. \n",
    "\n",
    "When we process text we must attend to the level of context we wish to consider for that word. As the examples given above show, the very same word \"humans\" in the first case, and \"keys\" in the second is understood in relation to context. \n",
    "\n",
    "Depending on the stability of the meaning of a word, more or less context might be required. Where less context is required we can say that a word's meaning is stable or unambiguous. Where more context is required we can say that a word's meaning is unstable or ambiguous. When we take in text, we want to be mindful that we are abstracting from context, but we want to ensure that we are not abstracting so far that our ability to understand meaning in the text is compromised.\n",
    "\n",
    "This has considerable consequences for our understanding of data. If we believe that hate speech is an unpleasant force in the world, we may be inclined to want to 'ban' hate speech. However, when doing so, we may find that people start to use different terms for the same sentiment. For example, while the word \"killed\" is now banned on TikTok, this has led to people using the term \"unalived\" to refer to the same 'sign' but with a different signifier. Similarly, on the subreddit /r/Stupidpol where ostensibly leftist reactionaries critique identity politics from their perspective, a ban on discussion of trans issues has led people to use the term \"train enthusiasts\" or \"model train operators\" to refer to members of the trans community. We see this all over the world. In China one classic example now is the use of the Grass Mud Horse (ËçâÊ≥•È©¨, c«éo n√≠ m«é) as a substitute for a vulgar curse word. While I won't get into details of the curse word (which you might infer), Horse in Chinese is m«é, whereas mother is mƒÅ. \n",
    "\n",
    "To that end, while we may have an interest in reducing _hate speech_, this is really an exercise in reducing _the performance of hated_. To that end, we are not trying to process hate-text, but use text to understand hate-language. This is important for social data science as it relates to our classic challenge of operationalisation and the related issue of construct validity. It also therefore can inform a number of decisions that we need to make in order to more fully appreciate the meaning behind words both _in context_ and _at scale_. It is also worth considering that within linguistics there is an entire subfield dedicated to \"pragmatics\" which is how the context of a word's use can influence its meaning. \n",
    "\n",
    "I mention this now because the first few techniques that we will show may superficially seem like they are very useful and powerful, but they end up being limited and preliminary. By being able to show these steps, we can then reflect on whether such steps are useful and needed in our own work. We also need to interrogate whether our work is sufficient to help inform others about any given research topic and about appropriate methods to address our _object of inquiry_. So let's proceed with some text processing, but please bear in mind that these tools are not the end of an analysis but the beginning. \n",
    "\n",
    "I also mention this because, and this is speaking in an editorial voice, I really dislike word clouds and wish to discourage their use in future academic work. While my rationale for this should be evident from the above discussion, I can spell it out more clearly: Word clouds divorce words from context, provide an inconsistent message to the viewer beyond \"big words important\", and make considerable assumptions that it is the words themselves and not how they are used which makes a difference. Imagine a word cloud for comments to an airline. In the middle we see \"Luggage\" and \"Service\". A superficial reading might say \"look how much people care about their luggage\" and \"people notice our commitment to good service\". When in fact the terms are there because most people are commenting to complain \"you lost my luggage and the service was terrible\". \n",
    "\n",
    "Where we want to end up is a place where we can make comments on our object of inquiry through text, but not simply comments on text. Thus, we will start with simple forms of text processing, but then see these as potential forms of pre-processing for more complex tasks. It is these more complex tasks that we ultimately want to consider when our skills get there."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Words as entities and thus tokens \n",
    "\n",
    "Words in English and in most languages can be considered as discrete elements. They may not be written this way. For example in Arabic words are chained together in written speech making such discretisation challenging. In English spaces are typically used to denote separate words. When there is no space we may consider it a compound word like \"something\" as a compound of \"some\" and \"thing\" or as portmonteau, where two words are blended such as \"hacktivism\". We typcially still consider compound words and portmonteaus as single words. English also has contractions where sometimes compounded words eliminate letters, such as _cannot_ be written as \"can't\" and _will not_ being written as \"won't\". From a semantic point of view \"cannot\" and \"can't\" may or may not be considered equivalent. In logical speech that may be the case, but in a peer reviewed paper, we would expect people to use expanded terms like cannot and avoid contractions which are seen as informal. \n",
    "\n",
    "In general the easist way to take English text and make the words discrete is to 'split' the words and to split them by space. Observe how I might split \"It was the best of times, it was the worst of times.\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It', 'was', 'the', 'best', 'of', 'times,', 'it', 'was', 'the', 'worst', 'of', 'times.']\n"
     ]
    }
   ],
   "source": [
    "quote = \"It was the best of times, it was the worst of times.\"\n",
    "print(quote.split(\" \")) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can also see that the words have punctuation. We might consider \"times,\" and \"times.\" to be the same word, but they do not use the same characters. Within English, punctuation inside a word (e.g., in \"can't\" or \"Y'know\") is usually seen as being a part of the word, whereas punctuation outside the word is not. Therefore, we can also remove these punctuation symbols from the words. Using standard python we can do the following: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['It', 'was', 'the', 'best', 'of', 'times', 'it', 'was', 'the', 'worst', 'of', 'times']\n"
     ]
    }
   ],
   "source": [
    "new_quote_list = [word.strip(string.punctuation) for word in quote.split(\" \")]\n",
    "print(new_quote_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we have words with the punctation stripped. Notice that in \"Introducing Python\" we introduced the `strip()` command to say how it can strip the whitespace from either side of a word, so that \" hello \" and \"hello\" are equivalent. But we can also strip any set of characters by providing them as an argument. By providing `string.punctuation`, we can strip this from both sides of the word. We could use `lstrip()` and `rstrip()` for the left and right sides respectively. However, we still have some words that are seen as generally comparable except for their capitalisation, such as \"It\" and \"it\". Capitalisation is important to consider when dealing with proper names. \"Mom\" and \"mom\" would refer to qualitatively different things, where \"Mom\" would be a title/name as in \"Mom said I could take the car\" whereas in the latter, \"Before taking the car you should ask your mom\". The latter is a role. There are many moms in the world, but for anyone there typically only one person referred to as \"Mom\". (Notably, within gay and lesbian parenting this issue comes up with words like \"Mom\" and \"Mama\" or \"Pop\" and \"Dad\" to distinguish specific parents). \n",
    "\n",
    "To that end, we can typically just iterate through words and make them lower or upper case in order to ensure that \"It\" and \"it\" are processed the same, but we also must be mindful that in doing so we might turn the one \"Mom\" into a more general \"mom\".  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lower_quote_list = [word.lower() for word in new_quote_list]\n",
    "print(lower_quote_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "English like many languages, but certainly not all, will change the way words are said or spelled depending on their grammatical context. Thus, the verb \"to buy\" could be spelled \"bought\" for the present participle such as \"He is _buying_ some shoes. He already _bought_ socks and then later he will _buy_ insoles\". In many cases we may want to consider these as equivalent. For this we cannot use simple mechanistic approaches (or rather such approaches are generally unsatisfying). For example, we could just `rstrip(\"ing\")` and `rstrip(\"s\")` from 'buying' to get 'buy' but what about 'bought'? \n",
    "\n",
    "This case is an example of how we will quickly start to run out of strategies with base Python and instead must consider existing models that can help guide us. Within specialist natural language packages we have two related notions: stemming and lemmatisation. Have a look at them below. In both cases we have to import a package to help out.\n",
    "\n",
    "The first thing to notice is that we do not need to do this discretisation process ourselves. In NLP it is called \"tokenisation\" and thus we can use a tokeniser for this. Observe: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt_tab', quiet=True) # quiet=True to suppress output if already downloaded. \n",
    "# Incidentally, Claude and Copilot insists it is `nltk.download('punkt')` which is out of date.\n",
    "# See: https://github.com/guardrails-ai/guardrails/issues/1013 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original tokens:\n",
      "['It', 'was', 'the', 'best', 'of', 'times', ',', 'it', 'was', 'the', 'worst', 'of', 'times', '.']\n",
      "Filtered tokens:\n",
      "['It', 'was', 'the', 'best', 'of', 'times', 'it', 'was', 'the', 'worst', 'of', 'times']\n"
     ]
    }
   ],
   "source": [
    "quote = \"It was the best of times, it was the worst of times.\"\n",
    "\n",
    "# Tokenize the quote\n",
    "tokens = word_tokenize(quote)\n",
    "\n",
    "print(\"Original tokens:\")\n",
    "print(tokens)\n",
    "\n",
    "# Filter out tokens that are exclusively punctuation or space\n",
    "filtered_tokens = [token for token in tokens if token not in string.punctuation and not token.isspace()]\n",
    "\n",
    "print(\"Filtered tokens:\")\n",
    "print(filtered_tokens)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stemming \n",
    "\n",
    "Stemming creates the \"stem\" for the words, such that \"officer\" and \"office\" become the same as \"offic\". Yet they are different words referring to different kinds of objects. Thus lemmatisation, where we find the root whole word may be more useful. On the other hand, lemmatisation in this case preserves the capitalisation and so \"The\" and \"the\" are distinct words. \n",
    "\n",
    "In both cases we downloaded a model with specific important details and used it to transform our data. They are simple models but they can help us in our work. And they can be more complex than one might think at first blush. For example, with lemmatisation we can also consider \"parts of speech\". One might officiate an event (as it be the speaker at the front of the room). That might make them an \"official\". In this case, they were considered distinct but in other cases such as \"run (verb) for office\" and \"that was a good run (noun)\", these should be considered differently.  Observe how we can integrate parts of speech using a POS tagger, i.e. `pos_tags = nltk.pos_tag(tokens)`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: The retired officer was officiating the event since he was the official representative of the head office\n",
      "\n",
      "Word-by-word comparison:\n",
      "Original        Stem            Lemma          \n",
      "---------------------------------------------\n",
      "The             the             The            \n",
      "retired         retir           retired        \n",
      "officer         offic           officer        \n",
      "was             wa              wa             \n",
      "officiating     offici          officiating    \n",
      "the             the             the            \n",
      "event           event           event          \n",
      "since           sinc            since          \n",
      "he              he              he             \n",
      "was             wa              wa             \n",
      "the             the             the            \n",
      "official        offici          official       \n",
      "representative  repres          representative \n",
      "of              of              of             \n",
      "the             the             the            \n",
      "head            head            head           \n",
      "office          offic           office         \n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Download required NLTK data\n",
    "nltk.download('wordnet', quiet=True)\n",
    "\n",
    "# Initialize stemmer and lemmatizer\n",
    "stemmer = PorterStemmer()\n",
    "lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "# Our test sentence\n",
    "text = \"The retired officer was officiating the event since he was the official representative of the head office\"\n",
    "\n",
    "# Tokenize the sentence\n",
    "tokens = word_tokenize(text)\n",
    "\n",
    "# Apply stemming and lemmatization\n",
    "stems = [stemmer.stem(word) for word in tokens]\n",
    "lemmas = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "# Print results in a formatted way\n",
    "print(\"Original:\", text)\n",
    "print(\"\\nWord-by-word comparison:\")\n",
    "print(f\"{'Original':<15} {'Stem':<15} {'Lemma':<15}\")\n",
    "print(\"-\" * 45)\n",
    "for orig, stem, lemma in zip(tokens, stems, lemmas):\n",
    "    print(f\"{orig:<15} {stem:<15} {lemma:<15}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above example we compare stemming and lemmatisation. Review the table to notice the difference. In the table below we use a separate model that should know which part of speech each word belongs to. Run the code and look at how the inclusion of parts of speech changes the lemmatisation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original: The retired officer was officiating the event since he was the official representative of the head office\n",
      "\n",
      "Word-by-word comparison:\n",
      "Original        POS      Basic Lemma     POS Lemma      \n",
      "------------------------------------------------------------\n",
      "The             DT       The             The            \n",
      "retired         JJ       retired         retired        \n",
      "officer         NN       officer         officer        \n",
      "was             VBD      wa              be             \n",
      "officiating     VBG      officiating     officiate      \n",
      "the             DT       the             the            \n",
      "event           NN       event           event          \n",
      "since           IN       since           since          \n",
      "he              PRP      he              he             \n",
      "was             VBD      wa              be             \n",
      "the             DT       the             the            \n",
      "official        JJ       official        official       \n",
      "representative  NN       representative  representative \n",
      "of              IN       of              of             \n",
      "the             DT       the             the            \n",
      "head            NN       head            head           \n",
      "office          NN       office          office         \n"
     ]
    }
   ],
   "source": [
    "# Download required NLTK data\n",
    "nltk.download('averaged_perceptron_tagger_eng', quiet=True)\n",
    "\n",
    "text = \"The retired officer was officiating the event since he was the official representative of the head office\"\n",
    "# text = \"The candidate had a good run but ran out of luck in the final run.\" # Uncomment to test with this sentence\n",
    "\n",
    "# Tokenize and get POS tags\n",
    "tokens = word_tokenize(text)\n",
    "pos_tags = nltk.pos_tag(tokens)\n",
    "\n",
    "# Basic lemmatization (without POS)\n",
    "basic_lemmas = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "\n",
    "# Lemmatization with POS tags\n",
    "# Convert Penn Treebank tags to WordNet POS tags\n",
    "def get_wordnet_pos(treebank_tag):\n",
    "    if treebank_tag.startswith('J'):\n",
    "        return 'a'  # adjective\n",
    "    elif treebank_tag.startswith('V'):\n",
    "        return 'v'  # verb\n",
    "    elif treebank_tag.startswith('N'):\n",
    "        return 'n'  # noun\n",
    "    elif treebank_tag.startswith('R'):\n",
    "        return 'r'  # adverb\n",
    "    else:\n",
    "        return 'n'  # noun as default\n",
    "\n",
    "pos_lemmas = [lemmatizer.lemmatize(word, get_wordnet_pos(pos)) \n",
    "              for word, pos in pos_tags]\n",
    "\n",
    "# Print results\n",
    "print(\"Original:\", text)\n",
    "print(\"\\nWord-by-word comparison:\")\n",
    "print(f\"{'Original':<15} {'POS':<8} {'Basic Lemma':<15} {'POS Lemma':<15}\")\n",
    "print(\"-\" * 60)\n",
    "for orig, (_, pos), basic_lem, pos_lem in zip(tokens, pos_tags, basic_lemmas, pos_lemmas):\n",
    "    print(f\"{orig:<15} {pos:<8} {basic_lem:<15} {pos_lem:<15}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By using a more complex model we are able to recover more of the context of any given word, such as its part of speech and the base verb \"be\" rather than \"was\". "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scoring text \n",
    "\n",
    "A common task with a suitably tokenised set of words is to assess sentiment. This means that we will have some model that will take in text and return a score reflecting the overall sentiment of some text. \n",
    "\n",
    "In my opinion, this sort of work often veers into an overly decontextualised understanding of text. Consequently, the scores that we get out of sentiment analysis are rarely very good for more than crude approximations. But it is also a good entry point into thinking again about more sophsiticated models because again they can enable more understanding of context. \n",
    "\n",
    "First, let's look at a very simple sentiment analysis library \"SimpleSentimentAnalyzer\". This is one is custom built and as you can see just has some positive words and negative words. It sums up the words present in the file. Then we can compare it to TextBlob which is also a very simple lexical analyser. Note when we say lexical here it means that we are operating on the words themselves. We are not converting words to some sort of abstraction and working on the abstraction. We will see that later when dealing with more complex models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Comparing sentiment analysis methods:\n",
      "\n",
      "Text: This is a good and wonderful day!\n",
      "Simple lexical score: 2\n",
      "TextBlob score: 0.85\n",
      "--------------------------------------------------\n",
      "Text: The movie was TERRIBLE and disappointing.\n",
      "Simple lexical score: -1\n",
      "TextBlob score: -0.80\n",
      "--------------------------------------------------\n",
      "Text: The food was okay, nothing special.\n",
      "Simple lexical score: 0\n",
      "TextBlob score: 0.43\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "\n",
    "class SimpleSentimentAnalyzer:\n",
    "    \"\"\"A basic lexical sentiment analyzer\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Very basic positive and negative word lists for demonstration\n",
    "        self.positive_words = {\n",
    "            'good', 'great', 'excellent', 'happy', 'wonderful', 'fantastic',\n",
    "            'amazing', 'love', 'best', 'beautiful', 'nice', 'perfect'\n",
    "        }\n",
    "        self.negative_words = {\n",
    "            'bad', 'terrible', 'awful', 'horrible', 'sad', 'wrong',\n",
    "            'hate', 'worst', 'poor', 'disappointing', 'negative', 'ugly'\n",
    "        }\n",
    "    \n",
    "    def analyze(self, text):\n",
    "        \"\"\"\n",
    "        Returns a simple sentiment score:\n",
    "        Positive words count - Negative words count\n",
    "        \"\"\"\n",
    "        words = text.lower().split()\n",
    "        positive_count = sum(1 for word in words if word in self.positive_words)\n",
    "        negative_count = sum(1 for word in words if word in self.negative_words)\n",
    "        return positive_count - negative_count\n",
    "\n",
    "def compare_methods(texts):\n",
    "    \"\"\"Compare simple lexical analysis with TextBlob\"\"\"\n",
    "    simple_analyzer = SimpleSentimentAnalyzer()\n",
    "    \n",
    "    print(\"Comparing sentiment analysis methods:\\n\")\n",
    "    for text in texts:\n",
    "        # Simple lexical analysis\n",
    "        simple_score = simple_analyzer.analyze(text)\n",
    "        \n",
    "        # TextBlob analysis\n",
    "        blob = TextBlob(text)\n",
    "        textblob_score = blob.sentiment.polarity\n",
    "        \n",
    "        print(f\"Text: {text}\")\n",
    "        print(f\"Simple lexical score: {simple_score}\")\n",
    "        print(f\"TextBlob score: {textblob_score:.2f}\")\n",
    "        print(\"-\" * 50)\n",
    "\n",
    "# Example usage\n",
    "example_texts = [\n",
    "    \"This is a good and wonderful day!\",\n",
    "    \"The movie was TERRIBLE and disappointing.\",\n",
    "    \"The food was okay, nothing special.\",\n",
    "]\n",
    "\n",
    "compare_methods(example_texts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the code below we compare textblob to a more sophisticated sentiment analyser called VADER by Hutto and Gilbert. This takes into account not just the words, but their frequency and some features such as whether they are in ALL CAPS. It was based on labelled data that was then fed into a static lexical model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results:\n",
      "                                               Text  Simple Score  TextBlob Score  VADER Compound  VADER Positive  VADER Negative  VADER Neutral\n",
      "0                 This is a good and wonderful day!             2           0.850           0.784           0.580           0.000          0.420\n",
      "1         The movie was terrible and disappointing.            -1          -0.800          -0.743           0.000           0.612          0.388\n",
      "2               The food was okay, nothing special.             0           0.429          -0.092           0.233           0.277          0.490\n",
      "3                           This is REALLY GREAT!!!             0           1.000           0.829           0.692           0.000          0.308\n",
      "4                      The book was not bad at all.            -1           0.350           0.431           0.322           0.000          0.678\n",
      "5  The service was good, but the food was terrible.             0          -0.150          -0.494           0.149           0.317          0.534\n",
      "\n",
      "Key observations:\n",
      "\n",
      "Text: This is a good and wonderful day!\n",
      "- Simple: 2 (just counts positive/negative words)\n",
      "- TextBlob: 0.85 (pattern-based lexical analysis)\n",
      "- VADER: 0.784 (rule-based with intensity)\n",
      "\n",
      "Text: The movie was terrible and disappointing.\n",
      "- Simple: -1 (just counts positive/negative words)\n",
      "- TextBlob: -0.8 (pattern-based lexical analysis)\n",
      "- VADER: -0.743 (rule-based with intensity)\n",
      "\n",
      "Text: The food was okay, nothing special.\n",
      "- Simple: 0 (just counts positive/negative words)\n",
      "- TextBlob: 0.429 (pattern-based lexical analysis)\n",
      "- VADER: -0.092 (rule-based with intensity)\n",
      "  Note: Significant difference between TextBlob and VADER scores!\n",
      "\n",
      "Text: This is REALLY GREAT!!!\n",
      "- Simple: 0 (just counts positive/negative words)\n",
      "- TextBlob: 1.0 (pattern-based lexical analysis)\n",
      "- VADER: 0.829 (rule-based with intensity)\n",
      "\n",
      "Text: The book was not bad at all.\n",
      "- Simple: -1 (just counts positive/negative words)\n",
      "- TextBlob: 0.35 (pattern-based lexical analysis)\n",
      "- VADER: 0.431 (rule-based with intensity)\n",
      "  Note: Simple analyzer doesn't handle negation properly!\n",
      "\n",
      "Text: The service was good, but the food was terrible.\n",
      "- Simple: 0 (just counts positive/negative words)\n",
      "- TextBlob: -0.15 (pattern-based lexical analysis)\n",
      "- VADER: -0.494 (rule-based with intensity)\n",
      "  Note: Significant difference between TextBlob and VADER scores!\n"
     ]
    }
   ],
   "source": [
    "from textblob import TextBlob\n",
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import pandas as pd\n",
    "\n",
    "class SimpleSentimentAnalyzer:\n",
    "    \"\"\"A basic lexical sentiment analyzer\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        # Very basic positive and negative word lists for demonstration\n",
    "        self.positive_words = {\n",
    "            'good', 'great', 'excellent', 'happy', 'wonderful', 'fantastic',\n",
    "            'amazing', 'love', 'best', 'beautiful', 'nice', 'perfect'\n",
    "        }\n",
    "        self.negative_words = {\n",
    "            'bad', 'terrible', 'awful', 'horrible', 'sad', 'wrong',\n",
    "            'hate', 'worst', 'poor', 'disappointing', 'negative', 'ugly'\n",
    "        }\n",
    "    \n",
    "    def analyze(self, text):\n",
    "        \"\"\"\n",
    "        Returns a simple sentiment score:\n",
    "        Positive words count - Negative words count\n",
    "        \"\"\"\n",
    "        words = text.lower().split()\n",
    "        positive_count = sum(1 for word in words if word in self.positive_words)\n",
    "        negative_count = sum(1 for word in words if word in self.negative_words)\n",
    "        return positive_count - negative_count\n",
    "\n",
    "def compare_sentiment_analyzers(texts):\n",
    "    \"\"\"Compare simple lexical analysis with TextBlob and VADER\"\"\"\n",
    "    # Initialize analyzers\n",
    "    simple_analyzer = SimpleSentimentAnalyzer()\n",
    "    vader_analyzer = SentimentIntensityAnalyzer()\n",
    "    \n",
    "    results = []\n",
    "    \n",
    "    for text in texts:\n",
    "        # Simple lexical analysis\n",
    "        simple_score = simple_analyzer.analyze(text)\n",
    "        \n",
    "        # TextBlob analysis\n",
    "        blob = TextBlob(text)\n",
    "        textblob_score = blob.sentiment.polarity\n",
    "        \n",
    "        # VADER analysis\n",
    "        vader_scores = vader_analyzer.polarity_scores(text)\n",
    "        \n",
    "        results.append({\n",
    "            'Text': text,\n",
    "            'Simple Score': simple_score,\n",
    "            'TextBlob Score': round(textblob_score, 3),\n",
    "            'VADER Compound': round(vader_scores['compound'], 3),\n",
    "            'VADER Positive': round(vader_scores['pos'], 3),\n",
    "            'VADER Negative': round(vader_scores['neg'], 3),\n",
    "            'VADER Neutral': round(vader_scores['neu'], 3)\n",
    "        })\n",
    "    \n",
    "    # Create DataFrame for nice display\n",
    "    df = pd.DataFrame(results)\n",
    "    return df\n",
    "\n",
    "# Example texts demonstrating different aspects of sentiment analysis\n",
    "example_texts = [\n",
    "    \"This is a good and wonderful day!\",\n",
    "    \"The movie was terrible and disappointing.\",\n",
    "    \"The food was okay, nothing special.\",\n",
    "    \"This is REALLY GREAT!!!\",  # Tests VADER's handling of caps and punctuation\n",
    "    \"The book was not bad at all.\",  # Tests handling of negation\n",
    "    \"The service was good, but the food was terrible.\",  # Tests mixed sentiment\n",
    "]\n",
    "\n",
    "# Run comparison\n",
    "results_df = compare_sentiment_analyzers(example_texts)\n",
    "print(\"\\nResults:\")\n",
    "print(results_df.to_string())\n",
    "\n",
    "# Detailed analysis of differences\n",
    "print(\"\\nKey observations:\")\n",
    "for idx, row in results_df.iterrows():\n",
    "    text = row['Text']\n",
    "    print(f\"\\nText: {text}\")\n",
    "    print(f\"- Simple: {row['Simple Score']} (just counts positive/negative words)\")\n",
    "    print(f\"- TextBlob: {row['TextBlob Score']} (pattern-based lexical analysis)\")\n",
    "    print(f\"- VADER: {row['VADER Compound']} (rule-based with intensity)\")\n",
    "    \n",
    "    # Highlight interesting differences\n",
    "    if abs(row['TextBlob Score'] - row['VADER Compound']) > 0.3:\n",
    "        print(\"  Note: Significant difference between TextBlob and VADER scores!\")\n",
    "        if \"!!!\" in text or text.isupper():\n",
    "            print(\"  (VADER is likely responding to emphasis from caps/punctuation)\")\n",
    "    if \"not\" in text.lower() and abs(row['Simple Score']) > 0:\n",
    "        print(\"  Note: Simple analyzer doesn't handle negation properly!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is just the tip of the scoring iceberg. For example, here are a few places we could go from here: \n",
    "- **OpenNLP**: A library from Apache which includes the ability to score text lexically using emotions. \n",
    "- **LIWC**: A huge and versitile API to scoring for all kinds of concepts such as \"anxiety\" or \"skepticism\". It started as a simple senitment analyser but has expanded considerably. \n",
    "- **Model-based scoring**: This is an active area of research, but we can generally get decent scores out of models like SentiBert which are trainined on vast corpus of texts with an architecture that reports a numeric sentiment score as the text passes through the model. Notably this is _not_ lexical scoring. It is not just giving the words scores but working on word embeddings, which is something we will explore later. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF: Understanding a corpus through the words used\n",
    "\n",
    "Departing from the earlier approach to scoring words based on a model that has some lexical scoring per word which is then transformed into a corpus-level score, we can also simply look to a corpus and determine what it means by the frequency of the words in the documents. A simple and yet highly versatile approach is TF-IDF: \n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "tf(t,d) &= \\frac{\\text{count of term }t\\text{ in document }d}{\\text{total number of terms in document }d} \\\\[10pt]\n",
    "idf(t) &= \\log\\left(\\frac{N}{1 + |\\{d \\in D: t \\in d\\}|}\\right) + 1 \\\\[10pt]\n",
    "tf\\text{-}idf(t,d,D) &= tf(t,d) \\times idf(t)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "In this forumula\n",
    "- $t$ is the term (our 'words')\n",
    "- $d$ is the document\n",
    "- $D$ is the corpus (collection of all documents)\n",
    "- $N$ is the total number of documents in the corpus\n",
    "- $|{d ‚àà D: t ‚àà d}|$ represents the number of documents containing the term $t$\n",
    "\n",
    "Term frequency gives us a weighted average of the term, not a count. We first look at how often a term appears given the number of other terms in the document. We will have one term frequency per-term per-document. This will be a 'term frequency matrix'. \n",
    "\n",
    "Inverse document frequency is a bit more tricky. It gives a weight to the presence of terms so that those which show up in most documents have a lower weight than those that show up in fewer documents. Note that $N$ is the numerator whereas the counting is in the denominator. This means that if $|{d ‚àà D: t ‚àà d}|$ is high it suggests these terms are in most of the docs, thus when we divide N by this count we will get a _lower_ score. Thus the term is more common and therefore makes the term less 'distinct' in any given document and thus probably less meaningful. \n",
    "\n",
    "Note that our TF matrix will have shape (num_documents √ó num_terms) while our IDF vector will have shape (num_terms,). When we multiply them, the IDF weights per-term will automatically be applied to every document's term frequencies.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Intermezzo: Matrix multiplication \n",
    "\n",
    "TF-IDF is then a matrix multiplication of the term frequency matrix by the inverse document frequency vector. A term with a high frequency in a few docs will therefore show up as more notable then a term with high frequency across all docs. In a way we are weighting the term frequencies per-document by how rare the terms are across all documents. \n",
    "\n",
    "The code below will perform this from scratch, but then we will see how we can abstract all these details in a 'Vectorizer'. \n",
    "\n",
    "The code below will produce relatively long output but the purpose is to see each of the transformation steps along the way. Let's run this code and then circle back to the functions that produce the various shapes and see if we can understand how we go to the end. \n",
    "\n",
    "Before we embark on this, here is a toy explanation of matrix multiplication with a vector. It gets a little less intuitive when we have multiply two matrices by each other. I again recommend the superlative series Essence of Linear Algebra by 3blue1brown for a potentially visual and intuitive understanding of these concepts. Here is the playlist: https://www.youtube.com/playlist?list=PLZHQObOWTQDPD3MizzM2xVFitgF8hE_ab \n",
    "\n",
    "Imagine a really simple example: \n",
    "\n",
    "~~~txt\n",
    "Term Frequencies:        cat   dog   sat\n",
    "Document 1:            [0.2   0.0   0.1]\n",
    "Document 2:            [0.1   0.3   0.0]\n",
    "\n",
    "IDF weights:           [2.0   1.5   3.0]\n",
    "\n",
    "Results in:\n",
    "\n",
    "TF-IDF Matrix:         cat   dog   sat\n",
    "Document 1:            [0.4   0.0   0.3]    # (0.2 √ó 2.0)  (0.0 √ó 1.5)  (0.1 √ó 3.0)\n",
    "Document 2:            [0.2   0.45  0.0]    # (0.1 √ó 2.0)  (0.3 √ó 1.5)  (0.0 √ó 3.0)\n",
    "~~~\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reiterate, the code below is a bit formidable if you read it from start to finish. I strongly recommend running it and reading through the output. Then going back to the cell and seeing if you can trace through the operations. They will be effectively: the sort of preprocessing we saw above and the matrix transformations as specified in the above formula. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting with corpus:\n",
      "Document 0: the cat sat on the mat\n",
      "Document 1: the dog chased the cat\n",
      "Document 2: the mat was on the floor\n",
      "Document 3: a dog and a cat played together\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Step 1: Tokenized corpus:\n",
      "Document 0: ['the', 'cat', 'sat', 'on', 'the', 'mat']\n",
      "Document 1: ['the', 'dog', 'chased', 'the', 'cat']\n",
      "Document 2: ['the', 'mat', 'was', 'on', 'the', 'floor']\n",
      "Document 3: ['a', 'dog', 'and', 'a', 'cat', 'played', 'together']\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Step 2: Vocabulary created:\n",
      "Vocabulary: ['a', 'and', 'cat', 'chased', 'dog', 'floor', 'mat', 'on', 'played', 'sat', 'the', 'together', 'was']\n",
      "\n",
      "Word to index mapping:\n",
      "a: 0\n",
      "and: 1\n",
      "cat: 2\n",
      "chased: 3\n",
      "dog: 4\n",
      "floor: 5\n",
      "mat: 6\n",
      "on: 7\n",
      "played: 8\n",
      "sat: 9\n",
      "the: 10\n",
      "together: 11\n",
      "was: 12\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Step 3: IDF values:\n",
      "        term  document_frequency       idf\n",
      "0          a                   1  1.693147\n",
      "1        and                   1  1.693147\n",
      "2        cat                   3  1.000000\n",
      "3     chased                   1  1.693147\n",
      "4        dog                   2  1.287682\n",
      "5      floor                   1  1.693147\n",
      "6        mat                   2  1.287682\n",
      "7         on                   2  1.287682\n",
      "8     played                   1  1.693147\n",
      "9        sat                   1  1.693147\n",
      "10       the                   3  1.000000\n",
      "11  together                   1  1.693147\n",
      "12       was                   1  1.693147\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Step 4: Term Frequency matrix:\n",
      "                   a       and       cat  chased       dog     floor       mat        on    played       sat       the  together       was\n",
      "Document 0  0.000000  0.000000  0.166667     0.0  0.000000  0.000000  0.166667  0.166667  0.000000  0.166667  0.333333  0.000000  0.000000\n",
      "Document 1  0.000000  0.000000  0.200000     0.2  0.200000  0.000000  0.000000  0.000000  0.000000  0.000000  0.400000  0.000000  0.000000\n",
      "Document 2  0.000000  0.000000  0.000000     0.0  0.000000  0.166667  0.166667  0.166667  0.000000  0.000000  0.333333  0.000000  0.166667\n",
      "Document 3  0.285714  0.142857  0.142857     0.0  0.142857  0.000000  0.000000  0.000000  0.142857  0.000000  0.000000  0.142857  0.000000\n",
      "\n",
      "================================================================================\n",
      "\n",
      "Step 5: TF-IDF matrix:\n",
      "                   a       and       cat    chased       dog     floor       mat        on    played       sat       the  together       was\n",
      "Document 0  0.000000  0.000000  0.166667  0.000000  0.000000  0.000000  0.214614  0.214614  0.000000  0.282191  0.333333  0.000000  0.000000\n",
      "Document 1  0.000000  0.000000  0.200000  0.338629  0.257536  0.000000  0.000000  0.000000  0.000000  0.000000  0.400000  0.000000  0.000000\n",
      "Document 2  0.000000  0.000000  0.000000  0.000000  0.000000  0.282191  0.214614  0.214614  0.000000  0.000000  0.333333  0.000000  0.282191\n",
      "Document 3  0.483756  0.241878  0.142857  0.000000  0.183955  0.000000  0.000000  0.000000  0.241878  0.000000  0.000000  0.241878  0.000000\n",
      "\n",
      "Step 6: Analyzing term distinctiveness:\n",
      "================================================================================\n",
      "\n",
      "Terms ranked by average TF-IDF score (most distinctive first):\n",
      "              term  avg_tfidf  max_tfidf      documents_with_max\n",
      "the            the   0.266667   0.400000              Document 1\n",
      "cat            cat   0.127381   0.200000              Document 1\n",
      "a                a   0.120939   0.483756              Document 3\n",
      "dog            dog   0.110373   0.257536              Document 1\n",
      "mat            mat   0.107307   0.214614  Document 0, Document 2\n",
      "on              on   0.107307   0.214614  Document 0, Document 2\n",
      "chased      chased   0.084657   0.338629              Document 1\n",
      "floor        floor   0.070548   0.282191              Document 2\n",
      "sat            sat   0.070548   0.282191              Document 0\n",
      "was            was   0.070548   0.282191              Document 2\n",
      "and            and   0.060470   0.241878              Document 3\n",
      "played      played   0.060470   0.241878              Document 3\n",
      "together  together   0.060470   0.241878              Document 3\n",
      "\n",
      "Top 3 most distinctive terms and where they appear strongest:\n",
      "\n",
      "the:\n",
      "  Average TF-IDF: 0.267\n",
      "  Maximum TF-IDF: 0.400\n",
      "  Strongest in: Document 1\n",
      "\n",
      "  TF-IDF scores across documents:\n",
      "    Document 0: 0.333\n",
      "    Document 1: 0.400\n",
      "    Document 2: 0.333\n",
      "    Document 3: 0.000\n",
      "\n",
      "cat:\n",
      "  Average TF-IDF: 0.127\n",
      "  Maximum TF-IDF: 0.200\n",
      "  Strongest in: Document 1\n",
      "\n",
      "  TF-IDF scores across documents:\n",
      "    Document 0: 0.167\n",
      "    Document 1: 0.200\n",
      "    Document 2: 0.000\n",
      "    Document 3: 0.143\n",
      "\n",
      "a:\n",
      "  Average TF-IDF: 0.121\n",
      "  Maximum TF-IDF: 0.484\n",
      "  Strongest in: Document 3\n",
      "\n",
      "  TF-IDF scores across documents:\n",
      "    Document 0: 0.000\n",
      "    Document 1: 0.000\n",
      "    Document 2: 0.000\n",
      "    Document 3: 0.484\n"
     ]
    }
   ],
   "source": [
    "# Sample corpus\n",
    "corpus = [\n",
    "    \"the cat sat on the mat\",\n",
    "    \"the dog chased the cat\",\n",
    "    \"the mat was on the floor\",\n",
    "    \"a dog and a cat played together\"\n",
    "]\n",
    "\n",
    "def tokenize(text):\n",
    "    \"\"\"Simple tokenizer that converts text to lowercase and splits on whitespace\"\"\"\n",
    "    text = text.lower()\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)\n",
    "    return text.split()\n",
    "\n",
    "def create_vocabulary(tokenized_corpus):\n",
    "    \"\"\"Create vocabulary and word-to-index mapping\"\"\"\n",
    "    unique_words = sorted(list(set(word for doc in tokenized_corpus for word in doc)))\n",
    "    word_to_idx = {word: idx for idx, word in enumerate(unique_words)}\n",
    "    return unique_words, word_to_idx\n",
    "\n",
    "def compute_tf(tokenized_doc, word_to_idx, vocab_size):\n",
    "    \"\"\"Compute Term Frequency for a document\"\"\"\n",
    "    word_counts = Counter(tokenized_doc)\n",
    "    tf_array = np.zeros(vocab_size)\n",
    "    for word, count in word_counts.items():\n",
    "        if word in word_to_idx:\n",
    "            idx = word_to_idx[word]\n",
    "            tf_array[idx] = count / len(tokenized_doc)\n",
    "    return tf_array\n",
    "\n",
    "def compute_idf(tokenized_corpus, word_to_idx, vocab_size):\n",
    "    \"\"\"Compute Inverse Document Frequency for all terms\"\"\"\n",
    "    doc_counts = np.zeros(vocab_size)\n",
    "    \n",
    "    for doc in tokenized_corpus:\n",
    "        unique_words = set(doc)\n",
    "        for word in unique_words:\n",
    "            if word in word_to_idx:\n",
    "                idx = word_to_idx[word]\n",
    "                doc_counts[idx] += 1\n",
    "    \n",
    "    idf = np.log(len(tokenized_corpus) / (1 + doc_counts)) + 1\n",
    "    return idf\n",
    "\n",
    "def visualize_tfidf_transformation(corpus):\n",
    "    \"\"\"\n",
    "    Compute TF-IDF matrix with visualization of intermediate steps\n",
    "    \"\"\"\n",
    "    print(\"Starting with corpus:\")\n",
    "    for i, doc in enumerate(corpus):\n",
    "        print(f\"Document {i}: {doc}\")\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "    # Step 1: Tokenize all documents\n",
    "    tokenized_corpus = [tokenize(doc) for doc in corpus]\n",
    "    print(\"Step 1: Tokenized corpus:\")\n",
    "    for i, doc in enumerate(tokenized_corpus):\n",
    "        print(f\"Document {i}: {doc}\")\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "    # Step 2: Create vocabulary\n",
    "    vocabulary, word_to_idx = create_vocabulary(tokenized_corpus)\n",
    "    print(\"Step 2: Vocabulary created:\")\n",
    "    print(\"Vocabulary:\", vocabulary)\n",
    "    print(\"\\nWord to index mapping:\")\n",
    "    for word, idx in word_to_idx.items():\n",
    "        print(f\"{word}: {idx}\")\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "    # Step 3: Compute document frequencies and IDF\n",
    "    vocab_size = len(vocabulary)\n",
    "    idf = compute_idf(tokenized_corpus, word_to_idx, vocab_size)\n",
    "    \n",
    "    # Create DataFrame for IDF values\n",
    "    idf_df = pd.DataFrame({\n",
    "        'term': vocabulary,\n",
    "        'document_frequency': [sum(1 for doc in tokenized_corpus if term in set(doc)) for term in vocabulary],\n",
    "        'idf': idf\n",
    "    })\n",
    "    print(\"Step 3: IDF values:\")\n",
    "    print(idf_df.to_string())\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "    # Step 4: Compute TF for each document\n",
    "    tf_matrix = np.zeros((len(corpus), vocab_size))\n",
    "    for i, doc in enumerate(tokenized_corpus):\n",
    "        tf_matrix[i] = compute_tf(doc, word_to_idx, vocab_size)\n",
    "    \n",
    "    # Create DataFrame for TF values\n",
    "    tf_df = pd.DataFrame(\n",
    "        tf_matrix,\n",
    "        columns=vocabulary,\n",
    "        index=[f\"Document {i}\" for i in range(len(corpus))]\n",
    "    )\n",
    "    print(\"Step 4: Term Frequency matrix:\")\n",
    "    print(tf_df.to_string())\n",
    "    print(\"\\n\" + \"=\"*80 + \"\\n\")\n",
    "\n",
    "    # Step 5: Compute TF-IDF matrix\n",
    "    tfidf_matrix = tf_matrix * idf\n",
    "    \n",
    "    # Create DataFrame for TF-IDF values\n",
    "    tfidf_df = pd.DataFrame(\n",
    "        tfidf_matrix,\n",
    "        columns=vocabulary,\n",
    "        index=[f\"Document {i}\" for i in range(len(corpus))]\n",
    "    )\n",
    "    print(\"Step 5: TF-IDF matrix:\")\n",
    "    print(tfidf_df.to_string())\n",
    "    \n",
    "    return {\n",
    "        'vocabulary': vocabulary,\n",
    "        'word_to_idx': word_to_idx,\n",
    "        'idf_df': idf_df,\n",
    "        'tf_df': tf_df,\n",
    "        'tfidf_df': tfidf_df\n",
    "    }\n",
    "\n",
    "def analyze_term_importance(tfidf_df):\n",
    "    \"\"\"\n",
    "    Analyze which terms are most distinctive across the corpus\n",
    "    \"\"\"\n",
    "    print(\"\\nStep 6: Analyzing term distinctiveness:\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Calculate average TF-IDF score for each term\n",
    "    term_importance = pd.DataFrame({\n",
    "        'term': tfidf_df.columns,\n",
    "        'avg_tfidf': tfidf_df.mean(),\n",
    "        'max_tfidf': tfidf_df.max(),\n",
    "        'documents_with_max': [\n",
    "            ', '.join([f\"Document {i}\" for i, value in enumerate(tfidf_df[term]) \n",
    "                      if value == tfidf_df[term].max()]) \n",
    "            for term in tfidf_df.columns\n",
    "        ]\n",
    "    }).sort_values('avg_tfidf', ascending=False)\n",
    "    \n",
    "    print(\"\\nTerms ranked by average TF-IDF score (most distinctive first):\")\n",
    "    print(term_importance.to_string())\n",
    "    \n",
    "    print(\"\\nTop 3 most distinctive terms and where they appear strongest:\")\n",
    "    for _, row in term_importance.head(3).iterrows():\n",
    "        print(f\"\\n{row['term']}:\")\n",
    "        print(f\"  Average TF-IDF: {row['avg_tfidf']:.3f}\")\n",
    "        print(f\"  Maximum TF-IDF: {row['max_tfidf']:.3f}\")\n",
    "        print(f\"  Strongest in: {row['documents_with_max']}\")\n",
    "        \n",
    "        # Show the term's TF-IDF scores across all documents\n",
    "        print(\"\\n  TF-IDF scores across documents:\")\n",
    "        for doc_idx, score in enumerate(tfidf_df[row['term']]):\n",
    "            print(f\"    Document {doc_idx}: {score:.3f}\")\n",
    "\n",
    "\n",
    "# Run the visualization\n",
    "results = visualize_tfidf_transformation(corpus)\n",
    "analyze_term_importance(results['tfidf_df'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our most distinctive terms in this document are \"the\" \"cat\" and \"a\". So this might not be the best example in that case since 'the' and 'a' are not usually meaningful. Rounding out the analyses, let's do the following: \n",
    "- consider \"stop words\"\n",
    "- look at how we can do all these operations using scikit-learn's TF-IDF vectorizer. \n",
    "\n",
    "Stop words are common English words that rarely convey meaning. That said, in my own work on /r/mensrights and /r/menslib the stop words for \"she\" and \"her\" turned out to be meaningful and helped to classify the subs. So this is an area where you would want to exercise your own judgment over the corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF scores after removing stopwords:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_dcf1c\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_dcf1c_level0_col0\" class=\"col_heading level0 col0\" >cat</th>\n",
       "      <th id=\"T_dcf1c_level0_col1\" class=\"col_heading level0 col1\" >chased</th>\n",
       "      <th id=\"T_dcf1c_level0_col2\" class=\"col_heading level0 col2\" >dog</th>\n",
       "      <th id=\"T_dcf1c_level0_col3\" class=\"col_heading level0 col3\" >floor</th>\n",
       "      <th id=\"T_dcf1c_level0_col4\" class=\"col_heading level0 col4\" >mat</th>\n",
       "      <th id=\"T_dcf1c_level0_col5\" class=\"col_heading level0 col5\" >played</th>\n",
       "      <th id=\"T_dcf1c_level0_col6\" class=\"col_heading level0 col6\" >sat</th>\n",
       "      <th id=\"T_dcf1c_level0_col7\" class=\"col_heading level0 col7\" >together</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_dcf1c_level0_row0\" class=\"row_heading level0 row0\" >Document 0</th>\n",
       "      <td id=\"T_dcf1c_row0_col0\" class=\"data row0 col0\" >0.448</td>\n",
       "      <td id=\"T_dcf1c_row0_col1\" class=\"data row0 col1\" >0.000</td>\n",
       "      <td id=\"T_dcf1c_row0_col2\" class=\"data row0 col2\" >0.000</td>\n",
       "      <td id=\"T_dcf1c_row0_col3\" class=\"data row0 col3\" >0.000</td>\n",
       "      <td id=\"T_dcf1c_row0_col4\" class=\"data row0 col4\" >0.553</td>\n",
       "      <td id=\"T_dcf1c_row0_col5\" class=\"data row0 col5\" >0.000</td>\n",
       "      <td id=\"T_dcf1c_row0_col6\" class=\"data row0 col6\" >0.702</td>\n",
       "      <td id=\"T_dcf1c_row0_col7\" class=\"data row0 col7\" >0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_dcf1c_level0_row1\" class=\"row_heading level0 row1\" >Document 1</th>\n",
       "      <td id=\"T_dcf1c_row1_col0\" class=\"data row1 col0\" >0.448</td>\n",
       "      <td id=\"T_dcf1c_row1_col1\" class=\"data row1 col1\" >0.702</td>\n",
       "      <td id=\"T_dcf1c_row1_col2\" class=\"data row1 col2\" >0.553</td>\n",
       "      <td id=\"T_dcf1c_row1_col3\" class=\"data row1 col3\" >0.000</td>\n",
       "      <td id=\"T_dcf1c_row1_col4\" class=\"data row1 col4\" >0.000</td>\n",
       "      <td id=\"T_dcf1c_row1_col5\" class=\"data row1 col5\" >0.000</td>\n",
       "      <td id=\"T_dcf1c_row1_col6\" class=\"data row1 col6\" >0.000</td>\n",
       "      <td id=\"T_dcf1c_row1_col7\" class=\"data row1 col7\" >0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_dcf1c_level0_row2\" class=\"row_heading level0 row2\" >Document 2</th>\n",
       "      <td id=\"T_dcf1c_row2_col0\" class=\"data row2 col0\" >0.000</td>\n",
       "      <td id=\"T_dcf1c_row2_col1\" class=\"data row2 col1\" >0.000</td>\n",
       "      <td id=\"T_dcf1c_row2_col2\" class=\"data row2 col2\" >0.000</td>\n",
       "      <td id=\"T_dcf1c_row2_col3\" class=\"data row2 col3\" >0.785</td>\n",
       "      <td id=\"T_dcf1c_row2_col4\" class=\"data row2 col4\" >0.619</td>\n",
       "      <td id=\"T_dcf1c_row2_col5\" class=\"data row2 col5\" >0.000</td>\n",
       "      <td id=\"T_dcf1c_row2_col6\" class=\"data row2 col6\" >0.000</td>\n",
       "      <td id=\"T_dcf1c_row2_col7\" class=\"data row2 col7\" >0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_dcf1c_level0_row3\" class=\"row_heading level0 row3\" >Document 3</th>\n",
       "      <td id=\"T_dcf1c_row3_col0\" class=\"data row3 col0\" >0.367</td>\n",
       "      <td id=\"T_dcf1c_row3_col1\" class=\"data row3 col1\" >0.000</td>\n",
       "      <td id=\"T_dcf1c_row3_col2\" class=\"data row3 col2\" >0.453</td>\n",
       "      <td id=\"T_dcf1c_row3_col3\" class=\"data row3 col3\" >0.000</td>\n",
       "      <td id=\"T_dcf1c_row3_col4\" class=\"data row3 col4\" >0.000</td>\n",
       "      <td id=\"T_dcf1c_row3_col5\" class=\"data row3 col5\" >0.575</td>\n",
       "      <td id=\"T_dcf1c_row3_col6\" class=\"data row3 col6\" >0.000</td>\n",
       "      <td id=\"T_dcf1c_row3_col7\" class=\"data row3 col7\" >0.575</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x1278771a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Most distinctive terms:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style type=\"text/css\">\n",
       "</style>\n",
       "<table id=\"T_e53f4\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th class=\"blank level0\" >&nbsp;</th>\n",
       "      <th id=\"T_e53f4_level0_col0\" class=\"col_heading level0 col0\" >term</th>\n",
       "      <th id=\"T_e53f4_level0_col1\" class=\"col_heading level0 col1\" >avg_tfidf</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th id=\"T_e53f4_level0_row0\" class=\"row_heading level0 row0\" >cat</th>\n",
       "      <td id=\"T_e53f4_row0_col0\" class=\"data row0 col0\" >cat</td>\n",
       "      <td id=\"T_e53f4_row0_col1\" class=\"data row0 col1\" >0.316</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e53f4_level0_row1\" class=\"row_heading level0 row1\" >mat</th>\n",
       "      <td id=\"T_e53f4_row1_col0\" class=\"data row1 col0\" >mat</td>\n",
       "      <td id=\"T_e53f4_row1_col1\" class=\"data row1 col1\" >0.293</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e53f4_level0_row2\" class=\"row_heading level0 row2\" >dog</th>\n",
       "      <td id=\"T_e53f4_row2_col0\" class=\"data row2 col0\" >dog</td>\n",
       "      <td id=\"T_e53f4_row2_col1\" class=\"data row2 col1\" >0.252</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e53f4_level0_row3\" class=\"row_heading level0 row3\" >floor</th>\n",
       "      <td id=\"T_e53f4_row3_col0\" class=\"data row3 col0\" >floor</td>\n",
       "      <td id=\"T_e53f4_row3_col1\" class=\"data row3 col1\" >0.196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th id=\"T_e53f4_level0_row4\" class=\"row_heading level0 row4\" >chased</th>\n",
       "      <td id=\"T_e53f4_row4_col0\" class=\"data row4 col0\" >chased</td>\n",
       "      <td id=\"T_e53f4_row4_col1\" class=\"data row4 col1\" >0.176</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n"
      ],
      "text/plain": [
       "<pandas.io.formats.style.Styler at 0x13af89f10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Download stopwords if you haven't already\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "# Get English stopwords from NLTK\n",
    "stop_words = list(set(stopwords.words('english')))\n",
    "\n",
    "# Initialize TF-IDF Vectorizer\n",
    "vectorizer = TfidfVectorizer(stop_words=stop_words)\n",
    "\n",
    "# Fit and transform the corpus\n",
    "tfidf_matrix = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# Convert to DataFrame for better visualization\n",
    "tfidf_df = pd.DataFrame(\n",
    "    tfidf_matrix.toarray(),\n",
    "    columns=vectorizer.get_feature_names_out(),\n",
    "    index=[f\"Document {i}\" for i in range(len(corpus))]\n",
    ")\n",
    "\n",
    "# Show the most distinctive terms\n",
    "term_importance = pd.DataFrame({\n",
    "    'term': tfidf_df.columns,\n",
    "    'avg_tfidf': tfidf_df.mean()\n",
    "}).sort_values('avg_tfidf', ascending=False)\n",
    "\n",
    "print(\"TF-IDF scores after removing stopwords:\")\n",
    "display(tfidf_df.style.format(\"{:.3f}\"))\n",
    "print(\"\\nMost distinctive terms:\")\n",
    "display(term_importance.head().style.format({'avg_tfidf': '{:.3f}'}))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Summary \n",
    "\n",
    "This lecture moved through language to text and then from text to ranking. We were able to rank words using a lexical approach that drew upon scoring with pre-defined scores to a structural approach that inductively used the word frequencies to give us a sense of the corpus. \n",
    "\n",
    "In the lab we apply this to headlines on Reddit. This will be the start of a cumulative group project on distinctive subreddits. The lab will give some TF-IDF results out of the box but your job will be to motivate an analysis and tweak a little code. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
